{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-01 15:31:33.036762: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-01 15:31:33.047930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-01 15:31:33.061594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-01 15:31:33.065448: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-01 15:31:33.076765: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-01 15:31:33.689723: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # Imposta l'ID della GPU che vuoi usare\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pdfplumber\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from llama_index.core.output_parsers import PydanticOutputParser\n",
    "from llama_parse import LlamaParse\n",
    "import transformers\n",
    "import re\n",
    "from IPython.display import display, Markdown\n",
    "import PyPDF2\n",
    "from haystack import Document\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Scarica i dati necessari per la tokenizzazione delle frasi\n",
    "\n",
    "# Inizializza il modello specificato\n",
    "model_rag = SentenceTransformer('paraphrase-multilingual-mpnet-base-v2', device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(entry, model=model_rag):\n",
    "    # Generazione dell'embedding\n",
    "    embedding = model.encode(entry, convert_to_numpy=True, normalize_embeddings=True)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a.alessandrelli/.local/lib/python3.10/site-packages/weaviate/warnings.py:162: DeprecationWarning: Dep016: Python client v3 `weaviate.Client(...)` connections and methods are deprecated. Update\n",
      "            your code to use Python client v4 `weaviate.WeaviateClient` connections and methods.\n",
      "\n",
      "            For Python Client v4 usage, see: https://weaviate.io/developers/weaviate/client-libraries/python\n",
      "            For code migration, see: https://weaviate.io/developers/weaviate/client-libraries/python/v3_v4_migration\n",
      "            \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import locale\n",
    "from datetime import datetime\n",
    "\n",
    "# Imposta la lingua italiana\n",
    "locale.setlocale(locale.LC_TIME, 'it_IT.UTF-8')\n",
    "key = 'NspY2XG4AgqNdA25wiP9jqyFChmrOXwh8tvI'\n",
    "url = \"https://atrawoytqmxwlvpuixika.c0.europe-west3.gcp.weaviate.cloud\"\n",
    "\n",
    "# client Weaviate\n",
    "client = weaviate.Client(\n",
    "    url=url,  \n",
    "    auth_client_secret=weaviate.auth.AuthApiKey(api_key=key), \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimozione di alcune stringhe di fine e inizio pagina\n",
    "def cleaning(text):\n",
    "    #text=re.sub(r'\\s+',' ',text)\n",
    "    text=re.sub(r'\\n+','\\n',text)\n",
    "    text=re.sub(r'Pagina \\d+ di \\d+|pagina \\d+ di \\d+|\\d+ di \\d+','\\n',text)\n",
    "    text=text.replace('Immagine di Archivio','')\n",
    "    text=text.replace('immagine generica illustrativa','')\n",
    "    text=text.replace('Image not found or type unknown','')\n",
    "    text=text.replace('I Imm agm\\ne\\na nog\\nt\\ni fn oue\\nn\\ndd oi\\nr\\nA tyr pc\\ne\\nh univ kni oo\\nwn','')\n",
    "    text=text.replace('logo FiscoOggi','')\n",
    "    if 'url:' in text.lower():\n",
    "        text=''\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf\n",
    "import os\n",
    "import pdfplumber\n",
    "\n",
    "DATA_DIR = [\"/home/a.alessandrelli/LLM/Hackaton_LLama_ROMA/DATI_DEFINITIVI/diritto-e-fisco\",\"/home/a.alessandrelli/LLM/Hackaton_LLama_ROMA/DATI_DEFINITIVI/pdf\",\"/home/a.alessandrelli/LLM/Hackaton_LLama_ROMA/DATI_DEFINITIVI/90k 100k\", \"/home/a.alessandrelli/LLM/Hackaton_LLama_ROMA/DATI_DEFINITIVI/pdf1\"]\n",
    "LOG_FILE = \"error_log.txt\"\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "for idx, c in enumerate(DATA_DIR):\n",
    "    for f in os.listdir(c):\n",
    "        files.append({f[:-4].strip():c+'/'+f})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1280"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_files(data_dir) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Ottiene i percorsi e i nomi dei file PDF nella directory specificata.\n",
    "\n",
    "    :param data_dir: Directory contenente i file PDF.\n",
    "    :return: Lista di dizionari con {\"ID\": \"filepath+filename\"}.\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for f in os.listdir(data_dir):\n",
    "        fname = os.path.join(data_dir, f)\n",
    "        if os.path.isfile(fname) and fname.endswith(\".pdf\") and not f.startswith(\"._\"):\n",
    "            # Aggiungi solo i file PDF che non iniziano con \"._\"\n",
    "            files.append({\"ID\": fname})\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertitore_data(data_testo):\n",
    "    formati = [\"%d %b %Y\", \"%d %B %Y\", \"%d/%m/%Y\", \"%Y-%m-%d\", \"%d-%m-%Y\"]\n",
    "\n",
    "    data_convertita = None\n",
    "    for formato in formati:\n",
    "        try:\n",
    "            data_convertita = datetime.strptime(data_testo, formato)\n",
    "            break\n",
    "        except ValueError:\n",
    "            pass\n",
    "    if data_convertita is None:\n",
    "        data_convertita = ''\n",
    "    return data_convertita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estrai_testo_pdfplumber(file_path1):\n",
    "    file_path = list(file_path1.values())[0]\n",
    "    controllo = -1\n",
    "    \n",
    "    testo_completo = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for pagina in pdf.pages:\n",
    "            try:\n",
    "                testo = pagina.extract_text()\n",
    "                if testo:  # Controlla se la pagina contiene testo\n",
    "                    testo_completo += testo + \"\\n\"\n",
    "            except:\n",
    "                continue\n",
    "    if not testo_completo.strip():  # Se non c'è testo estraibile\n",
    "        raise ValueError(f\"Nessun testo estraibile in {file_path}\")\n",
    "    text_temp = testo_completo.split('\\n')\n",
    "    text_clean = [cleaning(t.strip()) for t in text_temp if cleaning(t.strip()).strip()!='']\n",
    "    text_clean = [t for t in text_clean if 'Articolo pubblicato su FiscoOggi' not in t]\n",
    "    for pos, i in enumerate(text_clean):\n",
    "        if 'scritto da' in i.replace('\\n',''):\n",
    "            break\n",
    "    if pos < len(text_clean)/2:\n",
    "        titolo = '\\n'.join(text_clean[:pos])\n",
    "        text_clean1 ='\\n'.join(text_clean[pos+1:])\n",
    "        argomento = ' '\n",
    "    else:\n",
    "        titolo = ' '\n",
    "        text_clean1 ='\\n'.join(text_clean)\n",
    "        argomento = ' '\n",
    "    data = ' '\n",
    "    return  argomento, titolo, data, text_clean1.replace('I Imm agm\\ne\\na nog\\nt\\ni fn oue\\nn\\ndd oi\\nr\\nA tyr pc\\ne\\nh univ kni oo\\nwn\\n','').strip(), titolo+'.pdf',controllo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1280/1280 [06:56<00:00,  3.08it/s]\n"
     ]
    }
   ],
   "source": [
    "saving_path ='/home/a.alessandrelli/LLM/Hackaton_LLama_ROMA/dati_greco.jsonl'\n",
    "raw_docs_long=[]\n",
    "args=[]\n",
    "idx=0\n",
    "for f in tqdm(files[idx:]):\n",
    "    try:\n",
    "        temp = estrai_testo_pdfplumber(f)\n",
    "    except:\n",
    "        idx+=1\n",
    "        continue\n",
    "    argomento= temp[0]\n",
    "    title= temp[1]\n",
    "    data_1=temp[2]\n",
    "    testo=temp[3]\n",
    "    titolo_sporco = temp[4]\n",
    "    if temp[-1]==100:\n",
    "        continue\n",
    "    else:\n",
    "        raw_docs_long.append(Document(content=testo, meta={\"testo_completo\":testo, \"title\": title,\"argomento\":argomento,\"data\":str(data_1), \"fonte\":titolo_sporco}))\n",
    "        args.append({'text':testo, \"title\": title,\"argomento\":argomento,\"data\":str(data_1), \"fonte\":titolo_sporco})\n",
    "    if idx == 0:\n",
    "        mode ='w'\n",
    "    else:\n",
    "        mode ='a'\n",
    "    with open(saving_path,mode, encoding='utf-8') as file:\n",
    "        file.write(json.dumps(args[-1], ensure_ascii=False)+'\\n')\n",
    "    idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd=[]\n",
    "raw_docs_long=[]\n",
    "with open(saving_path, 'r', encoding='utf-8') as file:\n",
    "    for f in file:\n",
    "        temp =json.loads(f)\n",
    "        dd.append(temp)\n",
    "        testo=temp['text']\n",
    "        title=temp['title']\n",
    "        data_1=temp['data']\n",
    "        argomento=temp['argomento']\n",
    "        titolo_sporco=temp['fonte']\n",
    "        raw_docs_long.append(Document(content=testo, meta={\"testo_completo\":testo, \"title\": title,\"argomento\":argomento,\"data\":str(data_1), \"fonte\":titolo_sporco}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(id=99b2baf6faa5c31366e804fccb42f4239fc3dc0b422100aa3aadd51acd7e8079, content: 'Certificati medici falsi e non attendibili: come contestare la giusta\n",
       "causa di licenziamento.\n",
       "Presen...', meta: {'testo_completo': 'Certificati medici falsi e non attendibili: come contestare la giusta\\ncausa di licenziamento.\\nPresentare certificati medici falsi per giustificare assenze dal lavoro a\\ncausa della malattia di un figlio non giustifica un licenziamento, se il datore di\\nlavoro non riesce a dimostrare che il lavoratore fosse consapevole della non\\nautenticità di tali documenti.\\nNon spetta al dipendente verificare l’attendibilità dei certificati medici; il suo\\ncompito si limita solo alla trasmissione dei documenti al datore di lavoro.\\nQuest’ultimo, se ne rileva la falsità, è tenuto a provare che il lavoratore fosse a\\nconoscenza di tale non autenticità. Diversamente, il licenziamento per falsa\\nmalattia del figlio risulta illegittimo.\\nNon è possibile neanche presumere che l’uso di falsi certificati medici da parte del\\ndipendente implichi in maniera automatica la sua consapevolezza. Il semplice fatto\\nche il lavoratore sia il genitore del malato non comporta necessariamente che\\nconosca le effettive condizioni di salute del figlio e le discrepanze con quanto\\nriportato nei certificati.\\nQuesti principi sono stati affermati dalla Cassazione, con l’ordinanza numero\\n220891/2024 del 26 luglio, che ha confermato le pronunce emesse nei precedenti\\ngradi di giudizio.\\nLa Corte ha stabilito che la mancanza di prove circa la consapevolezza del\\nlavoratore sulla non genuinità dei certificati elimina i presupposti per una giusta\\ncausa di licenziamento. Si tratta, quindi, di un comportamento che non può\\nessere catalogato come illecito.\\nIl caso in questione ha origine dal licenziamento di un dipendente che, per\\ngiustificare le ripetute assenze dovute alla malattia del figlio, aveva\\npresentato certificazioni mediche risultate false. In particolare, due di questi\\ncertificati risultavano “perfettamente sovrapponibili”, avendo la stessa\\nriproduzione.\\nLa Corte d’Appello di Roma, confermando la decisione di primo grado, ha stabilito\\nche non era stata dimostrata la consapevolezza del lavoratore riguardo alla non\\nautenticità dei certificati medici. Inoltre, ha sostenuto che il solo utilizzo di tali\\ncertificati da parte del dipendente non poteva automaticamente indicare una sua\\nconsapevolezza della loro falsità.\\nLa Cassazione ha confermato tale interpretazione, respingendo l’argomentazione\\ndel datore di lavoro secondo cui sarebbe stato compito del lavoratore verificare\\nl’autenticità dei certificati medici utilizzati per giustificare le assenze.\\nSecondo la Corte di legittimità, l’onere della prova grava interamente sul\\ndatore di lavoro, in base alla regola generale dell’articolo 5 della Legge\\n604/1966, che assegna al datore il compito di dimostrare la giusta causa per il\\nlicenziamento.\\nSulla base di queste considerazioni, è stata confermata l’illegittimità del\\nlicenziamento, con conseguente reintegra del lavoratore nel suo posto\\nprecedente e il pagamento, a titolo di risarcimento, delle retribuzioni mensili non\\npercepite durante il periodo di assenza forzata.\\nLa decisione solleva alcune perplessità e invita a riflessioni critiche, in particolare\\nriguardo al fatto che il datore di lavoro non può avere alcun controllo sulla\\nproduzione dei certificati medici. È difficile comprendere come possa quest’ultimo\\ndimostrare la consapevolezza del lavoratore riguardo all’origine di certificati\\nmedici. Inoltre, risulta complesso capire come il datore possa avere un controllo\\neffettivo sulla veridicità dei certificati medici, e perché non si possa supporre che il\\nlavoratore, essendo direttamente coinvolto nella malattia del figlio, fosse a\\nconoscenza dell’incongruenza. Dopo tutto, il lavoratore è il primo a poter accertarsi\\ndella veridicità della documentazione medica.\\nLa Cassazione, decidendo di escludere la possibilità di presumere la\\nconsapevolezza del dipendente riguardo ai certificati falsi, non ha preso in\\nconsiderazione questa comune esperienza.', 'title': 'Licenziamento per falsa malattia\\ndel figlio', 'argomento': ' ', 'data': ' ', 'fonte': 'Licenziamento per falsa malattia\\ndel figlio.pdf'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_docs_long[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1266"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_docs_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57312"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs_long_greco=[]\n",
    "with open('/home/a.alessandrelli/LLM/AvvoChat/Raccolta_doc_index/dataset_completo_13_07.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for f in file:\n",
    "        temp=json.loads(f)\n",
    "        testo=temp['corpo']\n",
    "        title=temp['titolo']\n",
    "        data_1=' '\n",
    "        argomento=' '\n",
    "        titolo_sporco=' '\n",
    "        raw_docs_long_greco.append(Document(content=testo, meta={\"testo_completo\":testo, \"title\": title,\"argomento\":argomento,\"data\":str(data_1), \"fonte\":titolo_sporco}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_docs=DocumentSplitter(split_by=\"sentence\", split_length=5).run(raw_docs_long)['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8761"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vett_text = [r.content for r in raw_docs]\n",
    "vettorized_text = generate_embeddings(vett_text) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def rimuovi_caratteri_unicode(stringa):\n",
    "    stringa_normalizzata = unicodedata.normalize('NFKD', stringa)\n",
    "    stringa_ascii = stringa_normalizzata.encode('ascii', 'ignore').decode('ascii')\n",
    "    return stringa.replace(\"\\\\\\\\\",\"-\").replace(\"\\\\\",\"-\").replace(\"/\",\"-\").replace(\"//\",\"-\")\n",
    "\n",
    "\n",
    "raw_docs_vett = []\n",
    "raw_docs_vett_saving = []\n",
    "tot=0\n",
    "for idx,d in enumerate(raw_docs):\n",
    "    testo_originale=rimuovi_caratteri_unicode(str(raw_docs[idx].content))\n",
    "    \n",
    "    try:\n",
    "        meta = {\n",
    "                \"id_originale\" : raw_docs[idx].meta['source_id'],\n",
    "                \"text\":testo_originale,\n",
    "                \"testo_completo\" : raw_docs[idx].meta['testo_completo'],\n",
    "                \"titolo\":raw_docs[idx].meta['title'],\n",
    "                \"argomento\":raw_docs[idx].meta['argomento'],\n",
    "                \"data\":raw_docs[idx].meta['data'],\n",
    "                \"anno\":raw_docs[idx].meta['data'][0:4],\n",
    "                'font':raw_docs[idx].meta['fonte'],\n",
    "                }\n",
    "\n",
    "        embeddings = vettorized_text[idx]\n",
    "        raw_docs_vett.append({'text':meta, 'vector': embeddings})\n",
    "        raw_docs_vett_saving.append({'text':meta})\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Funzione per costruire il dict del documento in base alla nuova struttura JSONL\n",
    "def schema(d):\n",
    "    dict ={\n",
    "                \"id_originale\":d.get(\"id_originale\",\"\"),\n",
    "                \"text\":d.get(\"text\",\"\"),\n",
    "                \"titolo\":d.get(\"titolo\",\"\"),\n",
    "                \"testo_completo\":d.get(\"testo_completo\",\"\"),\n",
    "                \"argomento\":d.get(\"argomento\",\"\"),\n",
    "                \"anno\":d.get(\"anno\",\"\"),\n",
    "                \"data\":d.get(\"data\",\"\"),\n",
    "                'font':d.get(\"font\",\"\"),\n",
    "            }\n",
    "    return dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Tot:  8761 \n",
      "\n",
      "\n",
      "\n",
      "OK!!!!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def class_schema(name_class):\n",
    "    dict = {\n",
    "        \"class\": name_class,\n",
    "        \"vectorizer\": \"none\",\n",
    "    }\n",
    "    return dict\n",
    "\n",
    "\n",
    "\n",
    "if client.schema.exists('Fisco'):\n",
    "    tttt='ciao'\n",
    "else:  \n",
    "    class_cod = class_schema('Fisco')\n",
    "    client.schema.create_class(class_cod)\n",
    "    \n",
    "\n",
    "# Caricamento e indicizzazione dei dati\n",
    "print(\"\\n\\n Tot: \", len(raw_docs_vett), \"\\n\")\n",
    "\n",
    "\n",
    "client.batch.configure(batch_size=1500)  # Configure batch\n",
    "with client.batch as batch:\n",
    "    for i, d in enumerate(raw_docs_vett[34501:]):\n",
    "        try:\n",
    "            if (i % 1500 == 0):\n",
    "                print(f\"importing JSON block: {i + 1}\")\n",
    "            \n",
    "            properties = schema(d['text'])\n",
    "            batch.add_data_object(properties,class_name=\"Fisco\", vector=d['vector'])\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "            \n",
    "print(\"\\n\\nOK!!!!\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_greco=[]\n",
    "with open('/home/a.alessandrelli/LLM/AvvoChat/Raccolta_doc_index/codici_tot.jsonl', 'r', encoding='utf-8') as file:\n",
    "    for f in file:\n",
    "        dd_greco.append(json.loads(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'corpo': \"Costituzione della Repubblica Italiana: IL CAPO PROVVISORIO DELLO STATO Vista la deliberazione dell'Assemblea Costituente, che nella seduta del 22 dicembre 1947 ha approvato la Costituzione della Repubblica Italiana; Vista la XVIII disposizione finale della Costituzione; PROMULGA la Costituzione della Repubblica Italiana nel seguente testo: Art. 1 L'Italia è una Repubblica democratica, fondata sul lavoro La sovranità appartiene al popolo, che la esercita nelle forme e nei limiti della Costituzione \",\n",
       " 'titolo': 'Costituzione della Repubblica Italiana articolo 1'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd_greco[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
